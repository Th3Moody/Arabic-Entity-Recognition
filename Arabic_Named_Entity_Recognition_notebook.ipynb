{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Task3_playground.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "4YSvpD3A1BMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset (Google Colab Environment)\n",
        "! wget \"https://www.cs.cmu.edu/%7Eark/ArabicNER/AQMAR_Arabic_NER_corpus-1.0.zip\"\n",
        "! unzip \"/content/AQMAR_Arabic_NER_corpus-1.0.zip\" -d \"/content/corpus\""
      ],
      "metadata": {
        "id": "YfxaWopIzSD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entity Cleaner: Unites entity tags and fixs misspellings \n",
        "def tags_cleaner(entity):\n",
        "  entity = re.sub('\\n','',entity) # Remove the newline (\\n)\n",
        "  if entity in ['B-LOC', 'B-MIS', 'B-ORG','B-PER','I-LOC','I-MIS','I-ORG','I-PER','O']:\n",
        "    return entity\n",
        "  elif entity in ['B-MIS0','B-MIS1', 'B-MIS2', 'B-MIS3', 'B-MIS-1','B-MIS-2', 'B-MIS1`', 'B-MISS1']:\n",
        "    return 'B-MIS'\n",
        "  elif entity in ['I-MIS0','I-MIS1', 'I-MIS2', 'I-MIS3']:\n",
        "    return 'I-MIS'\n",
        "  elif entity in ['B-ENGLISH', 'B-SPANISH', 'OO', 'IO']:\n",
        "    return 'O'\n",
        "  elif entity == 'I--ORG':\n",
        "    return 'I-ORG'\n",
        "  else:\n",
        "    print('Error with entity:', entity)\n",
        "\n",
        "\n",
        "# Clean/Normalize Arabic Text\n",
        "def clean_str(text):\n",
        "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
        "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
        "    \n",
        "    # Remove tashkeel\n",
        "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "    text = re.sub(p_tashkeel,\"\", text)\n",
        "    \n",
        "    # Remove longation\n",
        "    p_longation = re.compile(r'(.)\\1+')\n",
        "    subst = r\"\\1\\1\"\n",
        "    text = re.sub(p_longation, subst, text)\n",
        "    \n",
        "    text = text.replace('وو', 'و')\n",
        "    text = text.replace('يي', 'ي')\n",
        "    text = text.replace('اا', 'ا')\n",
        "    \n",
        "    for i in range(0, len(search)):\n",
        "        text = text.replace(search[i], replace[i])\n",
        "    \n",
        "    # Trim    \n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Remove empty strings or strings that contains spaces only from sentences\n",
        "def re_clean(old_sentence, old_tags):\n",
        "  space_regex = re.compile(\"\\s+\")\n",
        "  new_sentence = []\n",
        "  new_tags = []\n",
        "  for j in range(len(old_sentence)):\n",
        "    # add word if not empty and doesn't contain spaces only\n",
        "    if old_sentence[j]!=\"\" and space_regex.match(old_sentence[j])==None:\n",
        "      new_sentence.append(old_sentence[j])\n",
        "      new_tags.append(old_tags[j])\n",
        "  \n",
        "  return new_sentence, new_tags\n"
      ],
      "metadata": {
        "id": "lHyhIIF0ofT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read sentences\n",
        "sentences = [] \n",
        "tags = []\n",
        "vocab = set()\n",
        "\n",
        "corpus_path = \"/content/corpus/\"\n",
        "for file in os.listdir(corpus_path):\n",
        "  if file.endswith('.txt'): # Get txt files only\n",
        "    print('Processing:', file)\n",
        "    topic = open(corpus_path+file)\n",
        "    sentence = []\n",
        "    entity = []\n",
        "    for line in topic.readlines():\n",
        "      if line == '\\n': # Sentence end\n",
        "        recleaned = re_clean(sentence, entity)\n",
        "        sentences.append(recleaned[0].copy())\n",
        "        tags.append(recleaned[1].copy())\n",
        "        sentence.clear()\n",
        "        entity.clear()\n",
        "      else:\n",
        "        line = line.split(sep=' ')\n",
        "        clean_word = clean_str(line[0])       # Cleaning word\n",
        "        vocab.add(clean_word)                 # Add word to the vocab\n",
        "        sentence.append(clean_word)           # Add the word\n",
        "        entity.append(tags_cleaner(line[1]))  # Clean and add entity\n",
        "\n",
        "\n",
        "print('Done [Sentences:', len(sentences), ', Tags:', len(tags), ', Unique Words:', len(vocab))"
      ],
      "metadata": {
        "id": "uInDRuaERS4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93722e89-e033-426c-930b-e0c3e1b588ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: Portugal_football_team.txt\n",
            "Processing: X_window_system.txt\n",
            "Processing: Atom.txt\n",
            "Processing: Computer_Software.txt\n",
            "Processing: Ibn_Tolun_Mosque.txt\n",
            "Processing: Islamic_Golden_Age.txt\n",
            "Processing: Periodic_Table.txt\n",
            "Processing: Enrico_Fermi.txt\n",
            "Processing: Summer_Olympics2004.txt\n",
            "Processing: Light.txt\n",
            "Processing: Physics.txt\n",
            "Processing: Internet.txt\n",
            "Processing: Islamic_History.txt\n",
            "Processing: Crusades.txt\n",
            "Processing: Damascus.txt\n",
            "Processing: Solaris.txt\n",
            "Processing: Raul_Gonzales.txt\n",
            "Processing: Ummaya_Mosque.txt\n",
            "Processing: Nuclear_Power.txt\n",
            "Processing: Imam_Hussein_Shrine.txt\n",
            "Processing: Linux.txt\n",
            "Processing: Razi.txt\n",
            "Processing: Real_Madrid.txt\n",
            "Processing: Football.txt\n",
            "Processing: Christiano_Ronaldo.txt\n",
            "Processing: Richard_Stallman.txt\n",
            "Processing: Computer.txt\n",
            "Processing: Soccer_Worldcup.txt\n",
            "Done [Sentences: 2687 , Tags: 2687 , Unique Words: 17481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a mapping betwween words and their IDs\n",
        "word2id = {word:id for  id, word in enumerate(vocab)}\n",
        "id2word = {id:word for  id, word in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "T4EKWDM599h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The unbalanced dataset problem**\n",
        "\n",
        "I found that:\n",
        "- max length of sentence is 290\n",
        "- 2156 out of 2687 sentences have 40 words or less (80.2% of the data)\n",
        "- 2485 out of 2687 sentences have 60 words or less (92.2% of the data)\n",
        "- Sentences have empty strings/words due to cleaning and were tagged 'O'\n",
        "- The 'O' tag represents 87.3% of the words\n",
        "\n",
        "----\n",
        "I re-cleaned the sentences again and removed all empty words and strings that contain spaces only. It made slight difference but it wasn't enough.\n",
        "\n",
        "Results:\n",
        "- max length of sentence is 271\n",
        "- 2240 out of 2687 sentences have 40 words or less (83.4% of the data)\n",
        "- 2514 out of 2687 sentences have 60 words or less (93.5% of the data)\n",
        "- No empty strings\n",
        "- The 'O' tag represents 86.4% of the words\n",
        "\n",
        "----\n",
        "\n",
        "I tried to find the ratio of tags according to sentence size. We made bins of size 20 words from 0 to 160 (8 bins)\n",
        "```\n",
        "# Percent of each tag per bin\n",
        "'B-LOC': [1.5, 2.1, 2.2, 2.9, 2.4, 2.4, 3.6, 1.6]\n",
        "'B-MIS': [5.2, 3.9, 3.5, 1.9, 1.6, 2.0, 2.2, 0.6]\n",
        "'B-ORG': [0.6, 0.6, 0.4, 0.9, 1.1, 1.6, 1.6, 1.6]\n",
        "'B-PER': [1.8, 1.9, 1.7, 3.1, 3.7, 4.2, 3.4, 4.7]\n",
        "'I-LOC': [0.6, 0.9, 1.1, 1.4, 0.8, 0.8, 1.9, 0.2]\n",
        "'I-MIS': [2.2, 2.2, 2.1, 1.3, 1.6, 2.1, 1.7, 0.6]\n",
        "'I-ORG': [0.7, 0.7, 0.5, 0.9, 0.9, 1.0, 1.7, 1.3]\n",
        "'I-PER': [0.9, 1.2, 1.0, 2.1, 2.5, 2.5, 2.0, 7.8]\n",
        "'O': [86.6, 86.6, 87.4, 85.5, 85.4, 83.5, 82.0, 81.5]\n",
        "```\n",
        "\n",
        "As it can be seen, all bins have the same distribution\n",
        "\n",
        "----\n",
        "Next step is to choose another padding size, I think size of 40 would be best as most sentences are 40 words or less."
      ],
      "metadata": {
        "id": "X-NX05ZOQnmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Sentence encoder\n",
        "def encode_sentence(old_sentence):\n",
        "  encoded_sentence = []\n",
        "  for word in old_sentence:\n",
        "    try:\n",
        "      encoded_sentence.append(word2id[word])\n",
        "    except KeyError:\n",
        "      encoded_sentence.append(0) # A dummy digit for out of vocab\n",
        "\n",
        "  return encoded_sentence\n",
        "\n",
        "# Encode Tags\n",
        "tags_encoding = {\n",
        "    'B-LOC':0,\n",
        "    'B-MIS':1,\n",
        "    'B-ORG':2,\n",
        "    'B-PER':3,\n",
        "    'I-LOC':4,\n",
        "    'I-MIS':5,\n",
        "    'I-ORG':6,\n",
        "    'I-PER':7,\n",
        "    'O':8\n",
        "  }\n",
        "def encode_tags(old_tags):\n",
        "  new_tags = [tags_encoding[tag] for tag in old_tags]\n",
        "  new_tags = to_categorical(y = new_tags, num_classes=9)\n",
        "  return new_tags"
      ],
      "metadata": {
        "id": "9B0jdBD9-a9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding\n",
        "sentences_encoded = []\n",
        "tags_encoded = []\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  sentences_encoded.append(encode_sentence(sentences[i]))\n",
        "  tags_encoded.append(encode_tags(tags[i]))"
      ],
      "metadata": {
        "id": "8p6QS6azAFp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Padding\n",
        "MAX_SEQUENCE_LENGTH = 40\n",
        "\n",
        "sentences_padded = pad_sequences(sequences = sentences_encoded, \n",
        "                                 maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                                 dtype='int32', \n",
        "                                 padding='post',\n",
        "                                 truncating='post',\n",
        "                                 value = 0)\n",
        "tags_padded = pad_sequences(sequences = tags_encoded, \n",
        "                                 maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                                 dtype='int32', \n",
        "                                 padding='post',\n",
        "                                 truncating='post',\n",
        "                                 value = np.array([0., 0., 0., 0., 0., 0., 0., 0., 1.]))"
      ],
      "metadata": {
        "id": "kKcTVg3uB84a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting data\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(sentences_padded, \n",
        "                                                                              tags_padded, \n",
        "                                                                              train_size=0.8, \n",
        "                                                                              random_state=42)"
      ],
      "metadata": {
        "id": "n3io8aspRJTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download AraVec (Word2Vec Model) by Abu Bakr Soliman, Kareem Eissa, and Samhaa R.El-Beltagy.\n",
        "! wget \"https://archive.org/download/aravec2.0/wiki_cbow_300.zip\"\n",
        "! unzip \"/content/wiki_cbow_300.zip\" -d \"/content/word2vec_model\""
      ],
      "metadata": {
        "id": "oFvpL__UI6Qk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d49b7da3-b6e5-46bb-d841-0580e876e7d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-18 21:05:17--  https://archive.org/download/aravec2.0/wiki_cbow_300.zip\n",
            "Resolving archive.org (archive.org)... 207.241.224.2\n",
            "Connecting to archive.org (archive.org)|207.241.224.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ia803107.us.archive.org/0/items/aravec2.0/wiki_cbow_300.zip [following]\n",
            "--2022-05-18 21:05:17--  https://ia803107.us.archive.org/0/items/aravec2.0/wiki_cbow_300.zip\n",
            "Resolving ia803107.us.archive.org (ia803107.us.archive.org)... 207.241.232.157\n",
            "Connecting to ia803107.us.archive.org (ia803107.us.archive.org)|207.241.232.157|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 364888893 (348M) [application/zip]\n",
            "Saving to: ‘wiki_cbow_300.zip.1’\n",
            "\n",
            "wiki_cbow_300.zip.1   4%[                    ]  16.49M   994KB/s    eta 1m 51s ^C\n",
            "Archive:  /content/wiki_cbow_300.zip\n",
            "replace /content/word2vec_model/wikipedia_cbow_300? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "# Load the Word2Vec model\n",
        "weights_path = \"/content/word2vec_model/wikipedia_cbow_300\"\n",
        "araVec = gensim.models.Word2Vec.load(weights_path)\n",
        "\n",
        "# Testing\n",
        "most_similar = araVec.wv.most_similar( \"محمد\" )\n",
        "for term, score in most_similar:\n",
        "\tprint(term, score)"
      ],
      "metadata": {
        "id": "RaMr0gIxvpus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c48a6606-2441-4dd3-d802-519f9f83ee95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "لمحمد 0.726012110710144\n",
            "احمد 0.7142194509506226\n",
            "عبدالرحمن 0.6745274066925049\n",
            "ابراهيم 0.6723851561546326\n",
            "مهدي 0.6686975955963135\n",
            "محمود 0.664846658706665\n",
            "يحي 0.637116551399231\n",
            "اسماعيل 0.6307213306427002\n",
            "حموده 0.6287057995796204\n",
            "عبدالحميد 0.6267551183700562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an embedding matrix for the embedding layer\n",
        "num_words = len(vocab)\n",
        "embed_size, = araVec['محمود'].shape\n",
        "embedding_matrix = np.zeros(shape=(num_words, embed_size))\n",
        "\n",
        "for word, id in word2id.items():\n",
        "  try:\n",
        "    embedding_matrix[id] = araVec[word]\n",
        "  except KeyError:\n",
        "    embedding_matrix[id] = np.zeros(embed_size)\n",
        "\n",
        "embedding_matrix.shape"
      ],
      "metadata": {
        "id": "MtwcA63dOEYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cafd9372-0e11-408b-8531-4e72ed8af874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17481, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DsgH0kRKecgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LSTM, Input, Dense, Embedding, TimeDistributed\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "\n",
        "tf.keras.backend.clear_session() # Makes sure old model was deleted if exists\n",
        "\n",
        "lstm_model = Sequential()\n",
        "# Adding Layers\n",
        "lstm_model.add(Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32'))\n",
        "lstm_model.add(Embedding(input_dim = len(vocab),              # Vocabulary Size (number of unique words for training)\n",
        "                        output_dim = embed_size,              # Length of the vector for each word (embedding dimension)\n",
        "                        input_length = MAX_SEQUENCE_LENGTH,   # Maximum length of a sequence\n",
        "                        weights = [embedding_matrix],         # Send the needed AraVec Weights\n",
        "                        trainable = False))\n",
        "\n",
        "lstm_model.add(LSTM(units = embed_size, \n",
        "                    return_sequences=True,\n",
        "                    dropout=0.5, \n",
        "                    recurrent_dropout=0.5))\n",
        "lstm_model.add(TimeDistributed(Dense(9, activation='softmax')))\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999), \n",
        "                   loss='categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "lstm_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTtLQW_3Npb4",
        "outputId": "fc849f1e-3e41-4106-8789-dab80ec63ca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 40, 300)           5244300   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 40, 300)           721200    \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 40, 9)            2709      \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,968,209\n",
            "Trainable params: 723,909\n",
            "Non-trainable params: 5,244,300\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.fit(train_sentences, \n",
        "               train_labels, \n",
        "               validation_split=0.15, \n",
        "               batch_size = 10,\n",
        "               epochs = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmUcYDjbVVP-",
        "outputId": "762944b0-8338-4c15-8336-df40e9bbaf3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "183/183 [==============================] - 63s 329ms/step - loss: 0.5704 - accuracy: 0.8823 - val_loss: 0.3908 - val_accuracy: 0.9214\n",
            "Epoch 2/10\n",
            "183/183 [==============================] - 59s 323ms/step - loss: 0.3443 - accuracy: 0.9227 - val_loss: 0.3085 - val_accuracy: 0.9238\n",
            "Epoch 3/10\n",
            "183/183 [==============================] - 59s 325ms/step - loss: 0.2891 - accuracy: 0.9268 - val_loss: 0.2611 - val_accuracy: 0.9279\n",
            "Epoch 4/10\n",
            "183/183 [==============================] - 60s 327ms/step - loss: 0.2555 - accuracy: 0.9310 - val_loss: 0.2302 - val_accuracy: 0.9329\n",
            "Epoch 5/10\n",
            "183/183 [==============================] - 59s 324ms/step - loss: 0.2308 - accuracy: 0.9348 - val_loss: 0.2100 - val_accuracy: 0.9364\n",
            "Epoch 6/10\n",
            "183/183 [==============================] - 60s 328ms/step - loss: 0.2126 - accuracy: 0.9387 - val_loss: 0.1955 - val_accuracy: 0.9389\n",
            "Epoch 7/10\n",
            "183/183 [==============================] - 59s 324ms/step - loss: 0.1994 - accuracy: 0.9409 - val_loss: 0.1852 - val_accuracy: 0.9425\n",
            "Epoch 8/10\n",
            "183/183 [==============================] - 60s 326ms/step - loss: 0.1880 - accuracy: 0.9438 - val_loss: 0.1759 - val_accuracy: 0.9442\n",
            "Epoch 9/10\n",
            "183/183 [==============================] - 62s 340ms/step - loss: 0.1788 - accuracy: 0.9464 - val_loss: 0.1699 - val_accuracy: 0.9450\n",
            "Epoch 10/10\n",
            "183/183 [==============================] - 60s 328ms/step - loss: 0.1713 - accuracy: 0.9476 - val_loss: 0.1644 - val_accuracy: 0.9482\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb1ee3e0790>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.evaluate(test_sentences, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiB21M7bY7bW",
        "outputId": "9409c6fc-6c44-4c0f-fa4d-918bdfedabda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - 1s 81ms/step - loss: 0.1675 - accuracy: 0.9481\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.16745567321777344, 0.9480947852134705]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_predict(sentence:str):\n",
        "  sentence = sentence.split(sep=' ')\n",
        "  # Keeping track of words so not to process 40 words every time\n",
        "  word_count = len(sentence) \n",
        "  # Clean sentence\n",
        "  ready_sentence = [clean_str(word) for word in sentence]\n",
        "  # Encode sentence\n",
        "  ready_sentence = encode_sentence(ready_sentence)\n",
        "  # Padding sentence\n",
        "  ready_sentence = pad_sequences(sequences = [ready_sentence], \n",
        "                                 maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                                 dtype='int32', \n",
        "                                 padding='post',\n",
        "                                 truncating='post',\n",
        "                                 value = 0)\n",
        "  \n",
        "  # Predict and return actual words only\n",
        "  predictions = lstm_model.predict(ready_sentence)[0][0:word_count]\n",
        "\n",
        "  i = 0\n",
        "  for prediction in predictions:\n",
        "    tags_onehot = {\n",
        "      'B-LOC':np.array([1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
        "      'B-MIS':np.array([0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
        "      'B-ORG':np.array([0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
        "      'B-PER':np.array([0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
        "      'I-LOC':np.array([0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
        "      'I-MIS':np.array([0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
        "      'I-ORG':np.array([0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
        "      'I-PER':np.array([0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
        "      'O':np.array([0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
        "    }\n",
        "    tags_scores = {\n",
        "      'B-LOC':0,\n",
        "      'B-MIS':0,\n",
        "      'B-ORG':0,\n",
        "      'B-PER':0,\n",
        "      'I-LOC':0,\n",
        "      'I-MIS':0,\n",
        "      'I-ORG':0,\n",
        "      'I-PER':0,\n",
        "      'O':0\n",
        "    }\n",
        "    for tag in list(tags_onehot.keys()):\n",
        "      tags_scores[tag] = np.linalg.norm(tags_onehot[tag] - prediction)\n",
        "    \n",
        "    \n",
        "    print(sentence[i],':',min(tags_scores, key=tags_scores.get))\n",
        "    i+=1\n",
        "\n"
      ],
      "metadata": {
        "id": "LRFJTTFCWir0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_predict('منشئ المسجد هو أحمد بن طولون مؤسس الدولة الطولونية في مصر والشام، تعود أصوله إلى قبيلة التغزغز التركية، وكانت أُسرته تقيم في بخاري.')"
      ],
      "metadata": {
        "id": "J37iOR2l1J8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa634e3-a614-446b-e239-5318df0ca57d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "منشئ : O\n",
            "المسجد : O\n",
            "هو : O\n",
            "أحمد : B-PER\n",
            "بن : I-PER\n",
            "طولون : I-PER\n",
            "مؤسس : O\n",
            "الدولة : B-LOC\n",
            "الطولونية : O\n",
            "في : O\n",
            "مصر : O\n",
            "والشام، : B-LOC\n",
            "تعود : O\n",
            "أصوله : O\n",
            "إلى : O\n",
            "قبيلة : O\n",
            "التغزغز : O\n",
            "التركية، : O\n",
            "وكانت : O\n",
            "أُسرته : O\n",
            "تقيم : O\n",
            "في : O\n",
            "بخاري. : B-LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_predict('محمود حسام ذهب الي مسجد')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg5UYVd3EqBb",
        "outputId": "86cf150a-8eba-4d1d-e05d-bba84611db1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "محمود : B-PER\n",
            "حسام : B-PER\n",
            "ذهب : O\n",
            "الي : O\n",
            "مسجد : O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_predict('عبدالرحمن خالد ذهب الي مسجد')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiZk03LxFD0k",
        "outputId": "2ac45072-53ce-4cf8-910f-591864a8ae40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "عبدالرحمن : O\n",
            "خالد : O\n",
            "ذهب : O\n",
            "الي : O\n",
            "مسجد : O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_predict('دمشق، هي عاصمة الجمهورية العربية السورية، ومركز محافظة دمشق. وهي إحدى أقدم مدن العالم مع تاريخ غير منقطع منذ أحد عشر ألف عام تقريبًا، وأقدم مدينة - عاصمة في العالم. أصبحت عاصمة منطقة سوريا منذ عام 635.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJyUyxVqFLpZ",
        "outputId": "ef4a5729-a23b-4565-f30c-fb114e7bec4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "دمشق، : B-LOC\n",
            "هي : O\n",
            "عاصمة : O\n",
            "الجمهورية : O\n",
            "العربية : O\n",
            "السورية، : O\n",
            "ومركز : O\n",
            "محافظة : O\n",
            "دمشق. : B-LOC\n",
            "وهي : O\n",
            "إحدى : O\n",
            "أقدم : O\n",
            "مدن : O\n",
            "العالم : O\n",
            "مع : O\n",
            "تاريخ : O\n",
            "غير : O\n",
            "منقطع : O\n",
            "منذ : O\n",
            "أحد : O\n",
            "عشر : O\n",
            "ألف : O\n",
            "عام : O\n",
            "تقريبًا، : O\n",
            "وأقدم : O\n",
            "مدينة : O\n",
            "- : O\n",
            "عاصمة : O\n",
            "في : O\n",
            "العالم. : O\n",
            "أصبحت : O\n",
            "عاصمة : O\n",
            "منطقة : O\n",
            "سوريا : B-LOC\n",
            "منذ : O\n",
            "عام : O\n",
            "635. : O\n"
          ]
        }
      ]
    }
  ]
}